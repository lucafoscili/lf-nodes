import numpy as np
import torch

from . import CATEGORY
from ...utils.constants import BLUE_CHANNEL_ID, FUNCTION, GREEN_CHANNEL_ID, Input, RED_CHANNEL_ID
from ...utils.helpers.api import get_resource_url
from ...utils.helpers.comfy import resolve_filepath, safe_send_sync
from ...utils.helpers.conversion import numpy_to_tensor, tensor_to_numpy, tensor_to_pil
from ...utils.helpers.logic import normalize_input_image, normalize_json_input, normalize_list_to_value, normalize_output_image
from ...utils.helpers.temp_cache import TempFileCache
from ...utils.helpers.ui import create_compare_node

# region LF_LUTApplication
class LF_LUTApplication:
    def __init__(self):
        self._temp_cache = TempFileCache()

    @classmethod
    def INPUT_TYPES(self):
        return {
            "required": {
                "image": (Input.IMAGE, {
                    "tooltip": "Target image to which the LUT will be applied."
                }),
                "lut_dataset": (Input.JSON, {
                    "tooltip": "LUT dataset generated by LUT Generation Node."
                }),
                "strength" : (Input.FLOAT, {
                    "default": 0.5,
                    "min": 0,
                    "max": 1,
                    "step": 0.05,
                    "tooltip": "The strength of the filter (from 0 - no effect, to 1 - full effect)."
                }),
            },
            "optional": {
                "preset": (Input.COMBO, {
                    "default": "auto_photoreal",
                    "options": ["auto_photoreal", "auto_anime", "highlights_safe", "aggressive", "legacy"],
                    "tooltip": "auto_photoreal: Balanced for photorealistic images with banding reduction and moderate neutral highlights.\nauto_anime: Optimized for stylized/anime images with stronger neutral highlights and lower banding threshold.\nhighlights_safe: Conservative approach with very strong highlight preservation and moderate banding protection.\naggressive: Full LUT application without banding reduction or highlight preservation.\nlegacy: Original behavior without enhancements."
                }),
                "ui_widget": (Input.LF_COMPARE, {
                    "default": {}
                })
            },
            "hidden": {
                "node_id": "UNIQUE_ID"
            }
        }

    CATEGORY = CATEGORY
    FUNCTION = FUNCTION
    OUTPUT_IS_LIST = (False, True)
    OUTPUT_NODE = True
    OUTPUT_TOOLTIPS = (
        "Adjusted image tensor.",
        "List of adjusted image tensors.",
        "Original image tensor.",
        "List of original image tensors.",
        "LUT applied to the image.",
        "List of LUTs applied to the images."
    )
    RETURN_NAMES = ("image", "image_list")
    RETURN_TYPES = (Input.IMAGE, Input.IMAGE)

    def on_exec(self, **kwargs: dict):
        self._temp_cache.cleanup()

        image: list[torch.Tensor] = normalize_input_image(kwargs.get("image", []))
        strength: float = normalize_list_to_value(kwargs.get("strength"))
        lut_dataset: dict = normalize_json_input(kwargs.get("lut_dataset", {}))
        preset: str = str(kwargs.get("preset", "auto_photoreal") or "auto_photoreal").lower()

        if preset in ("auto_photoreal", "photoreal", "auto"):
            smooth_interpolation = True
            preserve_neutral_highlights = True
            neutral_strength = 0.55
            auto_limit_strength = True
            banding_threshold = 0.15
            banding_max_reduction = 0.5
        elif preset in ("auto_anime", "anime", "stylized"):
            smooth_interpolation = True
            preserve_neutral_highlights = True
            neutral_strength = 0.65
            auto_limit_strength = True
            banding_threshold = 0.10
            banding_max_reduction = 0.7
        elif preset in ("highlights_safe", "safe"):
            smooth_interpolation = True
            preserve_neutral_highlights = True
            neutral_strength = 0.75
            auto_limit_strength = True
            banding_threshold = 0.12
            banding_max_reduction = 0.6
        elif preset in ("aggressive",):
            smooth_interpolation = True
            preserve_neutral_highlights = False
            neutral_strength = 0.0
            auto_limit_strength = False
            banding_threshold = 0.15  # not used
            banding_max_reduction = 0.5  # not used
        elif preset in ("legacy",):
            smooth_interpolation = False
            preserve_neutral_highlights = False
            neutral_strength = 0.0
            auto_limit_strength = False
            banding_threshold = 0.15  # not used
            banding_max_reduction = 0.5  # not used
        else:
            # Fallback to auto_photoreal
            smooth_interpolation = True
            preserve_neutral_highlights = True
            neutral_strength = 0.55
            auto_limit_strength = True
            banding_threshold = 0.15
            banding_max_reduction = 0.5

        nodes: list[dict] = []
        dataset: dict = { "nodes": nodes }

        if len(image) != len(lut_dataset):
            raise ValueError("Number of target images does not match the number of LUT datasets.")

        adjusted_images: list[torch.Tensor] = []
        for index, img in enumerate(image):
            lut = lut_dataset.get(f"Image #{index + 1}", None)
            if not lut:
                raise ValueError(f"LUT for Image #{index + 1} not found in dataset.")

            r = np.array([int(node["cells"][RED_CHANNEL_ID]["value"]) for node in lut["nodes"]], dtype=np.float32)
            g = np.array([int(node["cells"][GREEN_CHANNEL_ID]["value"]) for node in lut["nodes"]], dtype=np.float32)
            b = np.array([int(node["cells"][BLUE_CHANNEL_ID]["value"]) for node in lut["nodes"]], dtype=np.float32)

            image_np_float = tensor_to_numpy(img, dtype=np.float32) * 255.0
            adjusted_np_float = np.zeros_like(image_np_float, dtype=np.float32)

            if smooth_interpolation:
                xp = np.arange(256, dtype=np.float32)
                adjusted_np_float[:, :, 0] = np.interp(image_np_float[:, :, 0], xp, r)
                adjusted_np_float[:, :, 1] = np.interp(image_np_float[:, :, 1], xp, g)
                adjusted_np_float[:, :, 2] = np.interp(image_np_float[:, :, 2], xp, b)
            else:
                image_uint8 = np.clip(image_np_float, 0, 255).astype(np.uint8)
                adjusted_np_float[:, :, 0] = r[image_uint8[:, :, 0]]
                adjusted_np_float[:, :, 1] = g[image_uint8[:, :, 1]]
                adjusted_np_float[:, :, 2] = b[image_uint8[:, :, 2]]

            if preserve_neutral_highlights and neutral_strength > 0:
                # Luma estimate (Rec.709)
                luma = 0.2126 * adjusted_np_float[:, :, 0] + 0.7152 * adjusted_np_float[:, :, 1] + 0.0722 * adjusted_np_float[:, :, 2]
                # Near-neutral chroma check
                delta = (np.max(adjusted_np_float, axis=2) - np.min(adjusted_np_float, axis=2))
                mask = (luma >= 230.0) & (delta <= 12.0)
                if np.any(mask):
                    avg = adjusted_np_float.mean(axis=2, keepdims=True)
                    # Blend towards neutral average
                    adjusted_np_float[mask] = (
                        adjusted_np_float[mask] * (1.0 - neutral_strength) + avg[mask] * neutral_strength
                    )

            def _compute_banding_risk(rs: np.ndarray, gs: np.ndarray, bs: np.ndarray, img_float: np.ndarray) -> float:
                def channel_risk(arr: np.ndarray, channel_vals: np.ndarray) -> float:
                    d = np.diff(arr)
                    flat_mask = np.concatenate([d == 0, [False]])  # mark start of flat segment
                    flat_range_ratio = flat_mask.sum() / 256.0
                    intensities = np.clip(channel_vals, 0, 255).astype(np.int32)
                    flat_pixels_ratio = (flat_mask[intensities].sum() / intensities.size) if intensities.size else 0.0
                    return flat_range_ratio * flat_pixels_ratio
                r_channel_vals = img_float[:, :, 0]
                g_channel_vals = img_float[:, :, 1]
                b_channel_vals = img_float[:, :, 2]
                return float((channel_risk(rs, r_channel_vals) + channel_risk(gs, g_channel_vals) + channel_risk(bs, b_channel_vals)) / 3.0)

            banding_risk = _compute_banding_risk(r, g, b, image_np_float)

            effective_strength = strength
            if auto_limit_strength and banding_risk > banding_threshold and banding_max_reduction > 0:
                # Scale reduction proportionally above threshold
                excess = min(1.0, (banding_risk - banding_threshold) / max(1e-6, (1.0 - banding_threshold)))
                reduction = banding_max_reduction * excess
                effective_strength = strength * (1.0 - reduction)

            blended_np = (1 - effective_strength) * image_np_float + effective_strength * adjusted_np_float
            blended_np = np.clip(blended_np, 0, 255).astype(np.uint8)

            adjusted_tensor = numpy_to_tensor(blended_np)

            pil_image_original = tensor_to_pil(img)
            output_file_s, subfolder_s, filename_s = resolve_filepath(
                filename_prefix="lut_s",
                image=img,
            )
            pil_image_original.save(output_file_s, format="PNG")
            filename_s = get_resource_url(subfolder_s, filename_s, "temp")

            pil_image_blended = tensor_to_pil(adjusted_tensor)
            output_file_t, subfolder_t, filename_t = resolve_filepath(
                filename_prefix="lut_t",
                image=adjusted_tensor,
                temp_cache=self._temp_cache
            )
            pil_image_blended.save(output_file_t, format="PNG")
            filename_t = get_resource_url(subfolder_t, filename_t, "temp")

            adjusted_images.append(adjusted_tensor)
            nodes.append(create_compare_node(filename_s, filename_t, index))

        image_batch, image_list = normalize_output_image(adjusted_images)

        safe_send_sync("lutapplication", {
            "dataset": dataset
        }, kwargs.get("node_id"))

        return (image_batch[0], image_list)
# endregion

# region Mappings
NODE_CLASS_MAPPINGS = {
    "LF_LUTApplication": LF_LUTApplication,
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "LF_LUTApplication": "LUT Application (filter)",
}
# endregion
