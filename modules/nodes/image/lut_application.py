import numpy as np
import torch

from server import PromptServer

from . import CATEGORY
from ...utils.constants import BLUE_CHANNEL_ID, EVENT_PREFIX, FUNCTION, GREEN_CHANNEL_ID, Input, RED_CHANNEL_ID
from ...utils.helpers.api import get_resource_url
from ...utils.helpers.comfy import resolve_filepath
from ...utils.helpers.conversion import numpy_to_tensor, tensor_to_numpy, tensor_to_pil
from ...utils.helpers.logic import normalize_input_image, normalize_json_input, normalize_list_to_value, normalize_output_image
from ...utils.helpers.temp_cache import TempFileCache
from ...utils.helpers.ui import create_compare_node

# region LF_LUTApplication
class LF_LUTApplication:
    def __init__(self):
        self._temp_cache = TempFileCache()

    @classmethod
    def INPUT_TYPES(self):
        return {
            "required": {
                "image": (Input.IMAGE, {
                    "tooltip": "Target image to which the LUT will be applied."
                }),
                "lut_dataset": (Input.JSON, {
                    "tooltip": "LUT dataset generated by LUT Generation Node."
                }),
                "strength" : (Input.FLOAT, {
                    "default": 0.5, "min": 0, "max": 1, "step": 0.05,
                    "tooltip": "The strength of the filter (from 0 - no effect, to 1 - full effect)."
                }),
            },
            "optional": {
                "ui_widget": (Input.LF_COMPARE, {
                    "default": {}
                })
            },
            "hidden": {
                "node_id": "UNIQUE_ID"
            }
        }

    CATEGORY = CATEGORY
    FUNCTION = FUNCTION
    OUTPUT_IS_LIST = (False, True)
    OUTPUT_NODE = True
    RETURN_NAMES = ("image", "image_list")
    RETURN_TYPES = ("IMAGE", "IMAGE")

    def on_exec(self, **kwargs: dict):
        image: list[torch.Tensor] = normalize_input_image(kwargs.get("image", []))
        strength: float = normalize_list_to_value(kwargs.get("strength"))
        lut_dataset: dict = normalize_json_input(kwargs.get("lut_dataset", {}))

        nodes: list[dict] = []
        dataset: dict = { "nodes": nodes }

        if len(image) != len(lut_dataset):
            raise ValueError("Number of target images does not match the number of LUT datasets.")

        adjusted_images: list[torch.Tensor] = []
        for index, img in enumerate(image):
            lut = lut_dataset.get(f"Image #{index + 1}", None)
            if not lut:
                raise ValueError(f"LUT for Image #{index + 1} not found in dataset.")

            r = np.array([int(node["cells"][RED_CHANNEL_ID]["value"]) for node in lut["nodes"]], dtype=np.uint8)
            g = np.array([int(node["cells"][GREEN_CHANNEL_ID]["value"]) for node in lut["nodes"]], dtype=np.uint8)
            b = np.array([int(node["cells"][BLUE_CHANNEL_ID]["value"]) for node in lut["nodes"]], dtype=np.uint8)

            image_np = tensor_to_numpy(img)
            image_np = image_np.astype(np.uint8)
            adjusted_np = np.zeros_like(image_np)

            adjusted_np[:, :, 0] = r[image_np[:, :, 0]]
            adjusted_np[:, :, 1] = g[image_np[:, :, 1]]
            adjusted_np[:, :, 2] = b[image_np[:, :, 2]]

            image_np_float = image_np.astype(np.float32)
            adjusted_np_float = adjusted_np.astype(np.float32)

            blended_np = (1 - strength) * image_np_float + strength * adjusted_np_float
            blended_np = np.clip(blended_np, 0, 255).astype(np.uint8)

            adjusted_tensor = numpy_to_tensor(blended_np)

            pil_image_original = tensor_to_pil(img)
            output_file_s, subfolder_s, filename_s = resolve_filepath(
                filename_prefix="lut_s",
                image=img,
            )
            pil_image_original.save(output_file_s, format="PNG")
            filename_s = get_resource_url(subfolder_s, filename_s, "temp")

            pil_image_blended = tensor_to_pil(adjusted_tensor)
            output_file_t, subfolder_t, filename_t = resolve_filepath(
                filename_prefix="lut_t",
                image=adjusted_tensor,
                temp_cache=self._temp_cache
            )
            pil_image_blended.save(output_file_t, format="PNG")
            filename_t = get_resource_url(subfolder_t, filename_t, "temp")

            adjusted_images.append(adjusted_tensor)
            nodes.append(create_compare_node(filename_s, filename_t, index))

        image_batch, image_list = normalize_output_image(adjusted_images)

        PromptServer.instance.send_sync(f"{EVENT_PREFIX}lutapplication", {
            "node": kwargs.get("node_id"),
            "dataset": dataset
        })

        return (image_batch[0], image_list)
# endregion

# region Mappings
NODE_CLASS_MAPPINGS = {
    "LF_LUTApplication": LF_LUTApplication,
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "LF_LUTApplication": "LUT Application (filter)",
}
# endregion